{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Qmo37/VAE_MNIST/blob/master/VAE_MNIST.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "title"
      },
      "source": [
        "# VAE MNIST - 變分自編碼器手寫數字重建\n",
        "\n",
        "此筆記本展示如何使用變分自編碼器（Variational Autoencoder, VAE）來重建 MNIST 手寫數字。\n",
        "\n",
        "## 作業要求\n",
        "- ✅ 使用 MNIST 資料集\n",
        "- ✅ 實作 VAE (Encoder + Decoder)\n",
        "- ✅ Reparameterization trick\n",
        "- ✅ Adam 優化器\n",
        "- ✅ 顯示每個 epoch 平均損失\n",
        "- ✅ 輸出重建圖像"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "setup"
      },
      "source": [
        "## 1. 環境設置與套件導入"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "imports"
      },
      "outputs": [],
      "source": [
        "# 安裝必要套件（如果需要）\n",
        "# !pip install torch torchvision matplotlib\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import DataLoader\n",
        "from torchvision import datasets, transforms\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "import os\n",
        "\n",
        "# 設置設備\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "print(f'使用設備: {device}')\n",
        "\n",
        "# 設置隨機種子以確保結果可重現\n",
        "torch.manual_seed(42)\n",
        "np.random.seed(42)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "model"
      },
      "source": [
        "## 2. VAE 模型定義\n",
        "\n",
        "### 模型架構\n",
        "```\n",
        "編碼器: 784 → 400 → (mu=20, logvar=20)\n",
        "解碼器: 20 → 400 → 784\n",
        "```"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vae_class"
      },
      "outputs": [],
      "source": [
        "class VAE(nn.Module):\n",
        "    def __init__(self, input_dim=784, hidden_dim=400, latent_dim=20):\n",
        "        super(VAE, self).__init__()\n",
        "\n",
        "        # 編碼器: 784 -> 400 -> (mu, logvar)\n",
        "        self.fc1 = nn.Linear(input_dim, hidden_dim)\n",
        "        self.fc_mu = nn.Linear(hidden_dim, latent_dim)\n",
        "        self.fc_logvar = nn.Linear(hidden_dim, latent_dim)\n",
        "\n",
        "        # 解碼器: latent_dim -> 400 -> 784\n",
        "        self.fc3 = nn.Linear(latent_dim, hidden_dim)\n",
        "        self.fc4 = nn.Linear(hidden_dim, input_dim)\n",
        "\n",
        "    def encode(self, x):\n",
        "        \"\"\"編碼器：將輸入轉換為潛在空間參數\"\"\"\n",
        "        h1 = F.relu(self.fc1(x))\n",
        "        mu = self.fc_mu(h1)\n",
        "        logvar = self.fc_logvar(h1)\n",
        "        return mu, logvar\n",
        "\n",
        "    def reparameterize(self, mu, logvar):\n",
        "        \"\"\"重參數化技巧: z = mu + std * epsilon\"\"\"\n",
        "        std = torch.exp(0.5 * logvar)\n",
        "        eps = torch.randn_like(std)\n",
        "        return mu + eps * std\n",
        "\n",
        "    def decode(self, z):\n",
        "        \"\"\"解碼器：將潛在變數轉換回圖像\"\"\"\n",
        "        h3 = F.relu(self.fc3(z))\n",
        "        return torch.sigmoid(self.fc4(h3))\n",
        "\n",
        "    def forward(self, x):\n",
        "        mu, logvar = self.encode(x.view(-1, 784))\n",
        "        z = self.reparameterize(mu, logvar)\n",
        "        return self.decode(z), mu, logvar\n",
        "\n",
        "# 初始化模型\n",
        "model = VAE().to(device)\n",
        "print(f'模型參數數量: {sum(p.numel() for p in model.parameters()):,}')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "loss"
      },
      "source": [
        "## 3. 損失函數定義\n",
        "\n",
        "VAE 損失 = 重建損失 + KL 散度損失"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "loss_function"
      },
      "outputs": [],
      "source": [
        "def vae_loss(recon_x, x, mu, logvar):\n",
        "    \"\"\"\n",
        "    VAE 損失函數\n",
        "\n",
        "    Args:\n",
        "        recon_x: 重建的圖像\n",
        "        x: 原始圖像\n",
        "        mu: 潛在空間均值\n",
        "        logvar: 潛在空間對數方差\n",
        "    \"\"\"\n",
        "    # 重建損失 (Binary Cross Entropy)\n",
        "    BCE = F.binary_cross_entropy(recon_x, x.view(-1, 784), reduction='sum')\n",
        "\n",
        "    # KL 散度損失\n",
        "    KLD = -0.5 * torch.sum(1 + logvar - mu.pow(2) - logvar.exp())\n",
        "\n",
        "    return BCE + KLD"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "data"
      },
      "source": [
        "## 4. 資料載入 (MNIST)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "data_loading"
      },
      "outputs": [],
      "source": [
        "print(\"正在載入 MNIST 資料集...\")\n",
        "\n",
        "# 資料轉換\n",
        "transform = transforms.Compose([transforms.ToTensor()])\n",
        "\n",
        "# 載入資料集\n",
        "train_dataset = datasets.MNIST('./data', train=True, download=True, transform=transform)\n",
        "test_dataset = datasets.MNIST('./data', train=False, transform=transform)\n",
        "\n",
        "# 資料載入器\n",
        "train_loader = DataLoader(train_dataset, batch_size=128, shuffle=True)\n",
        "test_loader = DataLoader(test_dataset, batch_size=128, shuffle=False)\n",
        "\n",
        "print(f\"訓練資料: {len(train_dataset)} 張圖像\")\n",
        "print(f\"測試資料: {len(test_dataset)} 張圖像\")\n",
        "\n",
        "# 顯示一些樣本圖像\n",
        "def show_sample_images():\n",
        "    data_iter = iter(train_loader)\n",
        "    images, labels = next(data_iter)\n",
        "\n",
        "    fig, axes = plt.subplots(2, 5, figsize=(10, 4))\n",
        "    for i in range(10):\n",
        "        row, col = i // 5, i % 5\n",
        "        axes[row, col].imshow(images[i].squeeze(), cmap='gray')\n",
        "        axes[row, col].set_title(f'標籤: {labels[i].item()}')\n",
        "        axes[row, col].axis('off')\n",
        "\n",
        "    plt.suptitle('MNIST 樣本圖像')\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "\n",
        "show_sample_images()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "training"
      },
      "source": [
        "## 5. 訓練設置"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "training_setup"
      },
      "outputs": [],
      "source": [
        "# Adam 優化器 (作業要求)\n",
        "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
        "\n",
        "# 訓練函數\n",
        "def train(epoch):\n",
        "    model.train()\n",
        "    train_loss = 0\n",
        "\n",
        "    for batch_idx, (data, _) in enumerate(train_loader):\n",
        "        data = data.to(device)\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "        # 前向傳播\n",
        "        recon_batch, mu, logvar = model(data)\n",
        "\n",
        "        # 計算損失\n",
        "        loss = vae_loss(recon_batch, data, mu, logvar)\n",
        "\n",
        "        # 反向傳播\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        train_loss += loss.item()\n",
        "\n",
        "        # 進度顯示\n",
        "        if batch_idx % 200 == 0:\n",
        "            print(f'訓練 Epoch: {epoch} [{batch_idx * len(data)}/{len(train_loader.dataset)} '\n",
        "                  f'({100. * batch_idx / len(train_loader):.0f}%)]\\t'\n",
        "                  f'損失: {loss.item() / len(data):.6f}')\n",
        "\n",
        "    # 計算該 epoch 的平均損失\n",
        "    avg_loss = train_loss / len(train_loader.dataset)\n",
        "    print(f'====> Epoch: {epoch} 平均損失: {avg_loss:.4f}')\n",
        "    return avg_loss"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "run_training"
      },
      "source": [
        "## 6. 執行訓練 (5 個 epochs)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "training_loop"
      },
      "outputs": [],
      "source": [
        "print(\"開始訓練...\")\n",
        "print(\"=\"*50)\n",
        "\n",
        "epochs = 5  # 作業要求的 epoch 數\n",
        "losses = []\n",
        "\n",
        "for epoch in range(1, epochs + 1):\n",
        "    avg_loss = train(epoch)\n",
        "    losses.append(avg_loss)\n",
        "\n",
        "print(\"=\"*50)\n",
        "print(\"訓練完成！\")\n",
        "\n",
        "# 繪製損失曲線\n",
        "plt.figure(figsize=(10, 6))\n",
        "plt.plot(range(1, epochs + 1), losses, 'b-o', linewidth=2, markersize=8)\n",
        "plt.title('VAE 訓練損失曲線', fontsize=16)\n",
        "plt.xlabel('Epoch', fontsize=12)\n",
        "plt.ylabel('平均損失', fontsize=12)\n",
        "plt.grid(True, alpha=0.3)\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "print(f\"\\n📊 訓練總結:\")\n",
        "print(f\"• 最終損失: {losses[-1]:.4f}\")\n",
        "print(f\"• 損失改善: {losses[0] - losses[-1]:.4f}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "visualization"
      },
      "source": [
        "## 7. 結果視覺化\n",
        "\n",
        "顯示原始圖像與重建圖像的比較"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "results_viz"
      },
      "outputs": [],
      "source": [
        "print(\"生成重建結果...\")\n",
        "\n",
        "model.eval()\n",
        "with torch.no_grad():\n",
        "    # 取得測試資料\n",
        "    data, _ = next(iter(test_loader))\n",
        "    data = data.to(device)\n",
        "\n",
        "    # 選擇 8 個樣本進行視覺化\n",
        "    test_samples = data[:8]\n",
        "\n",
        "    # 重建\n",
        "    recon_samples, _, _ = model(test_samples)\n",
        "\n",
        "    # 創建比較圖\n",
        "    fig, axes = plt.subplots(2, 8, figsize=(16, 4))\n",
        "\n",
        "    for i in range(8):\n",
        "        # 原始圖像\n",
        "        axes[0, i].imshow(test_samples[i].cpu().view(28, 28), cmap='gray')\n",
        "        axes[0, i].set_title(f'原始 {i+1}', fontsize=10)\n",
        "        axes[0, i].axis('off')\n",
        "\n",
        "        # 重建圖像\n",
        "        axes[1, i].imshow(recon_samples[i].cpu().view(28, 28), cmap='gray')\n",
        "        axes[1, i].set_title(f'重建 {i+1}', fontsize=10)\n",
        "        axes[1, i].axis('off')\n",
        "\n",
        "    plt.suptitle('VAE MNIST 重建結果比較', fontsize=16, fontweight='bold')\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "\n",
        "print(\"✅ 重建結果生成完成！\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "generation"
      },
      "source": [
        "## 8. 額外展示：從潛在空間生成新圖像"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "generation_code"
      },
      "outputs": [],
      "source": [
        "# 從潛在空間隨機採樣生成新圖像\n",
        "model.eval()\n",
        "with torch.no_grad():\n",
        "    # 從標準正態分佈中採樣\n",
        "    z = torch.randn(16, 20).to(device)\n",
        "\n",
        "    # 解碼生成圖像\n",
        "    generated = model.decode(z)\n",
        "\n",
        "    # 顯示生成的圖像\n",
        "    fig, axes = plt.subplots(4, 4, figsize=(8, 8))\n",
        "\n",
        "    for i in range(16):\n",
        "        row, col = i // 4, i % 4\n",
        "        axes[row, col].imshow(generated[i].cpu().view(28, 28), cmap='gray')\n",
        "        axes[row, col].axis('off')\n",
        "\n",
        "    plt.suptitle('從潛在空間生成的新圖像', fontsize=16, fontweight='bold')\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "\n",
        "print(\"🎨 新圖像生成完成！\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "summary"
      },
      "source": [
        "## 9. 作業總結\n",
        "\n",
        "### ✅ 作業要求完成情況\n",
        "\n",
        "| 要求項目 | 完成狀態 | 說明 |\n",
        "|---------|---------|------|\n",
        "| 使用 MNIST 資料集 | ✅ | 自動下載並載入 MNIST 資料集 |\n",
        "| Encoder 實作 | ✅ | 784 → 400 → (mu, logvar) |\n",
        "| Reparameterization trick | ✅ | z = mu + std * epsilon |\n",
        "| Decoder 實作 | ✅ | 20 → 400 → 784 |\n",
        "| Adam 優化器 | ✅ | 學習率 0.001 |\n",
        "| 顯示 epoch 損失 | ✅ | 每個 epoch 顯示平均損失 |\n",
        "| 輸出重建圖像 | ✅ | 原始 vs 重建圖像比較 |\n",
        "\n",
        "### 📊 模型表現\n",
        "- **模型架構**: 簡潔有效的全連接網路\n",
        "- **訓練效率**: 5 epochs 快速收斂\n",
        "- **重建品質**: 清晰保留數字特徵\n",
        "- **程式碼**: 簡潔易懂，適合學習\n",
        "\n",
        "### 🎓 學習重點\n",
        "1. **VAE 核心概念**: 編碼器-解碼器架構\n",
        "2. **重參數化技巧**: 使反向傳播可行\n",
        "3. **損失函數**: 重建損失 + KL 散度\n",
        "4. **PyTorch 實作**: 模型定義、訓練、視覺化\n",
        "\n",
        "---\n",
        "\n",
        "**🎉 VAE MNIST 作業完成！**\n",
        "\n",
        "*這個實作展示了變分自編碼器的核心概念，程式碼簡潔且教育性強，適合深度學習入門。*"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}