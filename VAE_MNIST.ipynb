{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Qmo37/VAE_MNIST/blob/master/VAE_MNIST.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "title"
      },
      "source": [
        "# VAE MNIST - è®Šåˆ†è‡ªç·¨ç¢¼å™¨æ‰‹å¯«æ•¸å­—é‡å»º\n",
        "\n",
        "æ­¤ç­†è¨˜æœ¬å±•ç¤ºå¦‚ä½•ä½¿ç”¨è®Šåˆ†è‡ªç·¨ç¢¼å™¨ï¼ˆVariational Autoencoder, VAEï¼‰ä¾†é‡å»º MNIST æ‰‹å¯«æ•¸å­—ã€‚\n",
        "\n",
        "## ä½œæ¥­è¦æ±‚\n",
        "- âœ… ä½¿ç”¨ MNIST è³‡æ–™é›†\n",
        "- âœ… å¯¦ä½œ VAE (Encoder + Decoder)\n",
        "- âœ… Reparameterization trick\n",
        "- âœ… Adam å„ªåŒ–å™¨\n",
        "- âœ… é¡¯ç¤ºæ¯å€‹ epoch å¹³å‡æå¤±\n",
        "- âœ… è¼¸å‡ºé‡å»ºåœ–åƒ"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "setup"
      },
      "source": [
        "## 1. ç’°å¢ƒè¨­ç½®èˆ‡å¥—ä»¶å°å…¥"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "imports"
      },
      "outputs": [],
      "source": [
        "# å®‰è£å¿…è¦å¥—ä»¶ï¼ˆå¦‚æœéœ€è¦ï¼‰\n",
        "# !pip install torch torchvision matplotlib\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import DataLoader\n",
        "from torchvision import datasets, transforms\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "import os\n",
        "\n",
        "# è¨­ç½®è¨­å‚™\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "print(f'ä½¿ç”¨è¨­å‚™: {device}')\n",
        "\n",
        "# è¨­ç½®éš¨æ©Ÿç¨®å­ä»¥ç¢ºä¿çµæœå¯é‡ç¾\n",
        "torch.manual_seed(42)\n",
        "np.random.seed(42)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "model"
      },
      "source": [
        "## 2. VAE æ¨¡å‹å®šç¾©\n",
        "\n",
        "### æ¨¡å‹æ¶æ§‹\n",
        "```\n",
        "ç·¨ç¢¼å™¨: 784 â†’ 400 â†’ (mu=20, logvar=20)\n",
        "è§£ç¢¼å™¨: 20 â†’ 400 â†’ 784\n",
        "```"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vae_class"
      },
      "outputs": [],
      "source": [
        "class VAE(nn.Module):\n",
        "    def __init__(self, input_dim=784, hidden_dim=400, latent_dim=20):\n",
        "        super(VAE, self).__init__()\n",
        "\n",
        "        # ç·¨ç¢¼å™¨: 784 -> 400 -> (mu, logvar)\n",
        "        self.fc1 = nn.Linear(input_dim, hidden_dim)\n",
        "        self.fc_mu = nn.Linear(hidden_dim, latent_dim)\n",
        "        self.fc_logvar = nn.Linear(hidden_dim, latent_dim)\n",
        "\n",
        "        # è§£ç¢¼å™¨: latent_dim -> 400 -> 784\n",
        "        self.fc3 = nn.Linear(latent_dim, hidden_dim)\n",
        "        self.fc4 = nn.Linear(hidden_dim, input_dim)\n",
        "\n",
        "    def encode(self, x):\n",
        "        \"\"\"ç·¨ç¢¼å™¨ï¼šå°‡è¼¸å…¥è½‰æ›ç‚ºæ½›åœ¨ç©ºé–“åƒæ•¸\"\"\"\n",
        "        h1 = F.relu(self.fc1(x))\n",
        "        mu = self.fc_mu(h1)\n",
        "        logvar = self.fc_logvar(h1)\n",
        "        return mu, logvar\n",
        "\n",
        "    def reparameterize(self, mu, logvar):\n",
        "        \"\"\"é‡åƒæ•¸åŒ–æŠ€å·§: z = mu + std * epsilon\"\"\"\n",
        "        std = torch.exp(0.5 * logvar)\n",
        "        eps = torch.randn_like(std)\n",
        "        return mu + eps * std\n",
        "\n",
        "    def decode(self, z):\n",
        "        \"\"\"è§£ç¢¼å™¨ï¼šå°‡æ½›åœ¨è®Šæ•¸è½‰æ›å›åœ–åƒ\"\"\"\n",
        "        h3 = F.relu(self.fc3(z))\n",
        "        return torch.sigmoid(self.fc4(h3))\n",
        "\n",
        "    def forward(self, x):\n",
        "        mu, logvar = self.encode(x.view(-1, 784))\n",
        "        z = self.reparameterize(mu, logvar)\n",
        "        return self.decode(z), mu, logvar\n",
        "\n",
        "# åˆå§‹åŒ–æ¨¡å‹\n",
        "model = VAE().to(device)\n",
        "print(f'æ¨¡å‹åƒæ•¸æ•¸é‡: {sum(p.numel() for p in model.parameters()):,}')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "loss"
      },
      "source": [
        "## 3. æå¤±å‡½æ•¸å®šç¾©\n",
        "\n",
        "VAE æå¤± = é‡å»ºæå¤± + KL æ•£åº¦æå¤±"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "loss_function"
      },
      "outputs": [],
      "source": [
        "def vae_loss(recon_x, x, mu, logvar):\n",
        "    \"\"\"\n",
        "    VAE æå¤±å‡½æ•¸\n",
        "\n",
        "    Args:\n",
        "        recon_x: é‡å»ºçš„åœ–åƒ\n",
        "        x: åŸå§‹åœ–åƒ\n",
        "        mu: æ½›åœ¨ç©ºé–“å‡å€¼\n",
        "        logvar: æ½›åœ¨ç©ºé–“å°æ•¸æ–¹å·®\n",
        "    \"\"\"\n",
        "    # é‡å»ºæå¤± (Binary Cross Entropy)\n",
        "    BCE = F.binary_cross_entropy(recon_x, x.view(-1, 784), reduction='sum')\n",
        "\n",
        "    # KL æ•£åº¦æå¤±\n",
        "    KLD = -0.5 * torch.sum(1 + logvar - mu.pow(2) - logvar.exp())\n",
        "\n",
        "    return BCE + KLD"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "data"
      },
      "source": [
        "## 4. è³‡æ–™è¼‰å…¥ (MNIST)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "data_loading"
      },
      "outputs": [],
      "source": [
        "print(\"æ­£åœ¨è¼‰å…¥ MNIST è³‡æ–™é›†...\")\n",
        "\n",
        "# è³‡æ–™è½‰æ›\n",
        "transform = transforms.Compose([transforms.ToTensor()])\n",
        "\n",
        "# è¼‰å…¥è³‡æ–™é›†\n",
        "train_dataset = datasets.MNIST('./data', train=True, download=True, transform=transform)\n",
        "test_dataset = datasets.MNIST('./data', train=False, transform=transform)\n",
        "\n",
        "# è³‡æ–™è¼‰å…¥å™¨\n",
        "train_loader = DataLoader(train_dataset, batch_size=128, shuffle=True)\n",
        "test_loader = DataLoader(test_dataset, batch_size=128, shuffle=False)\n",
        "\n",
        "print(f\"è¨“ç·´è³‡æ–™: {len(train_dataset)} å¼µåœ–åƒ\")\n",
        "print(f\"æ¸¬è©¦è³‡æ–™: {len(test_dataset)} å¼µåœ–åƒ\")\n",
        "\n",
        "# é¡¯ç¤ºä¸€äº›æ¨£æœ¬åœ–åƒ\n",
        "def show_sample_images():\n",
        "    data_iter = iter(train_loader)\n",
        "    images, labels = next(data_iter)\n",
        "\n",
        "    fig, axes = plt.subplots(2, 5, figsize=(10, 4))\n",
        "    for i in range(10):\n",
        "        row, col = i // 5, i % 5\n",
        "        axes[row, col].imshow(images[i].squeeze(), cmap='gray')\n",
        "        axes[row, col].set_title(f'æ¨™ç±¤: {labels[i].item()}')\n",
        "        axes[row, col].axis('off')\n",
        "\n",
        "    plt.suptitle('MNIST æ¨£æœ¬åœ–åƒ')\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "\n",
        "show_sample_images()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "training"
      },
      "source": [
        "## 5. è¨“ç·´è¨­ç½®"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "training_setup"
      },
      "outputs": [],
      "source": [
        "# Adam å„ªåŒ–å™¨ (ä½œæ¥­è¦æ±‚)\n",
        "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
        "\n",
        "# è¨“ç·´å‡½æ•¸\n",
        "def train(epoch):\n",
        "    model.train()\n",
        "    train_loss = 0\n",
        "\n",
        "    for batch_idx, (data, _) in enumerate(train_loader):\n",
        "        data = data.to(device)\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "        # å‰å‘å‚³æ’­\n",
        "        recon_batch, mu, logvar = model(data)\n",
        "\n",
        "        # è¨ˆç®—æå¤±\n",
        "        loss = vae_loss(recon_batch, data, mu, logvar)\n",
        "\n",
        "        # åå‘å‚³æ’­\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        train_loss += loss.item()\n",
        "\n",
        "        # é€²åº¦é¡¯ç¤º\n",
        "        if batch_idx % 200 == 0:\n",
        "            print(f'è¨“ç·´ Epoch: {epoch} [{batch_idx * len(data)}/{len(train_loader.dataset)} '\n",
        "                  f'({100. * batch_idx / len(train_loader):.0f}%)]\\t'\n",
        "                  f'æå¤±: {loss.item() / len(data):.6f}')\n",
        "\n",
        "    # è¨ˆç®—è©² epoch çš„å¹³å‡æå¤±\n",
        "    avg_loss = train_loss / len(train_loader.dataset)\n",
        "    print(f'====> Epoch: {epoch} å¹³å‡æå¤±: {avg_loss:.4f}')\n",
        "    return avg_loss"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "run_training"
      },
      "source": [
        "## 6. åŸ·è¡Œè¨“ç·´ (5 å€‹ epochs)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "training_loop"
      },
      "outputs": [],
      "source": [
        "print(\"é–‹å§‹è¨“ç·´...\")\n",
        "print(\"=\"*50)\n",
        "\n",
        "epochs = 5  # ä½œæ¥­è¦æ±‚çš„ epoch æ•¸\n",
        "losses = []\n",
        "\n",
        "for epoch in range(1, epochs + 1):\n",
        "    avg_loss = train(epoch)\n",
        "    losses.append(avg_loss)\n",
        "\n",
        "print(\"=\"*50)\n",
        "print(\"è¨“ç·´å®Œæˆï¼\")\n",
        "\n",
        "# ç¹ªè£½æå¤±æ›²ç·š\n",
        "plt.figure(figsize=(10, 6))\n",
        "plt.plot(range(1, epochs + 1), losses, 'b-o', linewidth=2, markersize=8)\n",
        "plt.title('VAE è¨“ç·´æå¤±æ›²ç·š', fontsize=16)\n",
        "plt.xlabel('Epoch', fontsize=12)\n",
        "plt.ylabel('å¹³å‡æå¤±', fontsize=12)\n",
        "plt.grid(True, alpha=0.3)\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "print(f\"\\nğŸ“Š è¨“ç·´ç¸½çµ:\")\n",
        "print(f\"â€¢ æœ€çµ‚æå¤±: {losses[-1]:.4f}\")\n",
        "print(f\"â€¢ æå¤±æ”¹å–„: {losses[0] - losses[-1]:.4f}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "visualization"
      },
      "source": [
        "## 7. çµæœè¦–è¦ºåŒ–\n",
        "\n",
        "é¡¯ç¤ºåŸå§‹åœ–åƒèˆ‡é‡å»ºåœ–åƒçš„æ¯”è¼ƒ"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "results_viz"
      },
      "outputs": [],
      "source": [
        "print(\"ç”Ÿæˆé‡å»ºçµæœ...\")\n",
        "\n",
        "model.eval()\n",
        "with torch.no_grad():\n",
        "    # å–å¾—æ¸¬è©¦è³‡æ–™\n",
        "    data, _ = next(iter(test_loader))\n",
        "    data = data.to(device)\n",
        "\n",
        "    # é¸æ“‡ 8 å€‹æ¨£æœ¬é€²è¡Œè¦–è¦ºåŒ–\n",
        "    test_samples = data[:8]\n",
        "\n",
        "    # é‡å»º\n",
        "    recon_samples, _, _ = model(test_samples)\n",
        "\n",
        "    # å‰µå»ºæ¯”è¼ƒåœ–\n",
        "    fig, axes = plt.subplots(2, 8, figsize=(16, 4))\n",
        "\n",
        "    for i in range(8):\n",
        "        # åŸå§‹åœ–åƒ\n",
        "        axes[0, i].imshow(test_samples[i].cpu().view(28, 28), cmap='gray')\n",
        "        axes[0, i].set_title(f'åŸå§‹ {i+1}', fontsize=10)\n",
        "        axes[0, i].axis('off')\n",
        "\n",
        "        # é‡å»ºåœ–åƒ\n",
        "        axes[1, i].imshow(recon_samples[i].cpu().view(28, 28), cmap='gray')\n",
        "        axes[1, i].set_title(f'é‡å»º {i+1}', fontsize=10)\n",
        "        axes[1, i].axis('off')\n",
        "\n",
        "    plt.suptitle('VAE MNIST é‡å»ºçµæœæ¯”è¼ƒ', fontsize=16, fontweight='bold')\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "\n",
        "print(\"âœ… é‡å»ºçµæœç”Ÿæˆå®Œæˆï¼\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "generation"
      },
      "source": [
        "## 8. é¡å¤–å±•ç¤ºï¼šå¾æ½›åœ¨ç©ºé–“ç”Ÿæˆæ–°åœ–åƒ"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "generation_code"
      },
      "outputs": [],
      "source": [
        "# å¾æ½›åœ¨ç©ºé–“éš¨æ©Ÿæ¡æ¨£ç”Ÿæˆæ–°åœ–åƒ\n",
        "model.eval()\n",
        "with torch.no_grad():\n",
        "    # å¾æ¨™æº–æ­£æ…‹åˆ†ä½ˆä¸­æ¡æ¨£\n",
        "    z = torch.randn(16, 20).to(device)\n",
        "\n",
        "    # è§£ç¢¼ç”Ÿæˆåœ–åƒ\n",
        "    generated = model.decode(z)\n",
        "\n",
        "    # é¡¯ç¤ºç”Ÿæˆçš„åœ–åƒ\n",
        "    fig, axes = plt.subplots(4, 4, figsize=(8, 8))\n",
        "\n",
        "    for i in range(16):\n",
        "        row, col = i // 4, i % 4\n",
        "        axes[row, col].imshow(generated[i].cpu().view(28, 28), cmap='gray')\n",
        "        axes[row, col].axis('off')\n",
        "\n",
        "    plt.suptitle('å¾æ½›åœ¨ç©ºé–“ç”Ÿæˆçš„æ–°åœ–åƒ', fontsize=16, fontweight='bold')\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "\n",
        "print(\"ğŸ¨ æ–°åœ–åƒç”Ÿæˆå®Œæˆï¼\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "summary"
      },
      "source": [
        "## 9. ä½œæ¥­ç¸½çµ\n",
        "\n",
        "### âœ… ä½œæ¥­è¦æ±‚å®Œæˆæƒ…æ³\n",
        "\n",
        "| è¦æ±‚é …ç›® | å®Œæˆç‹€æ…‹ | èªªæ˜ |\n",
        "|---------|---------|------|\n",
        "| ä½¿ç”¨ MNIST è³‡æ–™é›† | âœ… | è‡ªå‹•ä¸‹è¼‰ä¸¦è¼‰å…¥ MNIST è³‡æ–™é›† |\n",
        "| Encoder å¯¦ä½œ | âœ… | 784 â†’ 400 â†’ (mu, logvar) |\n",
        "| Reparameterization trick | âœ… | z = mu + std * epsilon |\n",
        "| Decoder å¯¦ä½œ | âœ… | 20 â†’ 400 â†’ 784 |\n",
        "| Adam å„ªåŒ–å™¨ | âœ… | å­¸ç¿’ç‡ 0.001 |\n",
        "| é¡¯ç¤º epoch æå¤± | âœ… | æ¯å€‹ epoch é¡¯ç¤ºå¹³å‡æå¤± |\n",
        "| è¼¸å‡ºé‡å»ºåœ–åƒ | âœ… | åŸå§‹ vs é‡å»ºåœ–åƒæ¯”è¼ƒ |\n",
        "\n",
        "### ğŸ“Š æ¨¡å‹è¡¨ç¾\n",
        "- **æ¨¡å‹æ¶æ§‹**: ç°¡æ½”æœ‰æ•ˆçš„å…¨é€£æ¥ç¶²è·¯\n",
        "- **è¨“ç·´æ•ˆç‡**: 5 epochs å¿«é€Ÿæ”¶æ–‚\n",
        "- **é‡å»ºå“è³ª**: æ¸…æ™°ä¿ç•™æ•¸å­—ç‰¹å¾µ\n",
        "- **ç¨‹å¼ç¢¼**: ç°¡æ½”æ˜“æ‡‚ï¼Œé©åˆå­¸ç¿’\n",
        "\n",
        "### ğŸ“ å­¸ç¿’é‡é»\n",
        "1. **VAE æ ¸å¿ƒæ¦‚å¿µ**: ç·¨ç¢¼å™¨-è§£ç¢¼å™¨æ¶æ§‹\n",
        "2. **é‡åƒæ•¸åŒ–æŠ€å·§**: ä½¿åå‘å‚³æ’­å¯è¡Œ\n",
        "3. **æå¤±å‡½æ•¸**: é‡å»ºæå¤± + KL æ•£åº¦\n",
        "4. **PyTorch å¯¦ä½œ**: æ¨¡å‹å®šç¾©ã€è¨“ç·´ã€è¦–è¦ºåŒ–\n",
        "\n",
        "---\n",
        "\n",
        "**ğŸ‰ VAE MNIST ä½œæ¥­å®Œæˆï¼**\n",
        "\n",
        "*é€™å€‹å¯¦ä½œå±•ç¤ºäº†è®Šåˆ†è‡ªç·¨ç¢¼å™¨çš„æ ¸å¿ƒæ¦‚å¿µï¼Œç¨‹å¼ç¢¼ç°¡æ½”ä¸”æ•™è‚²æ€§å¼·ï¼Œé©åˆæ·±åº¦å­¸ç¿’å…¥é–€ã€‚*"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}